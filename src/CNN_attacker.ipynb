{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "TDJ5xM0GH7nc",
        "FZCJaX71BSXr"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Pre-processing phase**\n"
      ],
      "metadata": {
        "id": "UeDT8O3iI6GN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the needed modules throughout the whole experiment"
      ],
      "metadata": {
        "id": "25cdE2TNJThZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os \n",
        "import seaborn\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "#np.random.seed(42)  "
      ],
      "metadata": {
        "id": "raDhGfUIJL61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create folders in which we will upload the needed files for the experiments"
      ],
      "metadata": {
        "id": "f-TFeSZvJcIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment if you want to remove all csv files in \"out/\" directory\n",
        "#!rm  out/*.csv\n",
        "# uncomment if you want to remove all csv files in \"out_div/\" directory\n",
        "#!rm  out_div/*.csv\n",
        "# uncomment if you want to remove all csv files in \"kmeans/\" directory\n",
        "#!rm  kmeans/*kmeans\n",
        "!mkdir out\n",
        "!mkdir out_div\n",
        "!mkdir kmeans\n",
        "!pwd"
      ],
      "metadata": {
        "id": "FusePCkIJSHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define function which parse, load and create training, validation and test sets, given the path of the files "
      ],
      "metadata": {
        "id": "MVkNn3N_UIH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### \n",
        "# upload files directly from tab on \n",
        "#   the left\n",
        "####\n",
        "def load_dataset(path):\n",
        "    '''\n",
        "    @arg: path contains the path in which there are the \n",
        "        files to be parsed, loaded and split\n",
        "\n",
        "    @return: num_classes\n",
        "             num_samples\n",
        "             num_pkt\n",
        "             num_features\n",
        "             x_train\n",
        "             x_valid\n",
        "             x_test\n",
        "             y_train\n",
        "             y_test\n",
        "             y_valid\n",
        "             d_labels\n",
        "             dataset\n",
        "             label_dataset\n",
        "    '''\n",
        "\n",
        "    path_normal_data = path\n",
        "\n",
        "    num_classes =  -1   #n° of different traffic flows\n",
        "    num_samples =  -1   #n° of different video samples taken\n",
        "    num_pkt =      -1   #n° pkt intercepted in traffic flow\n",
        "    num_features = -1   #n° features (e.g., global_size, pkt_byte, inter-times, ...)\n",
        "    x_train = None\n",
        "    x_valid = None\n",
        "    x_test  = None\n",
        "    y_train = []  \n",
        "    y_test  = []  \n",
        "    y_valid = []  \n",
        "\n",
        "    bypassed_videos = []\n",
        "\n",
        "    d_labels = {} #dictionary which contains encoding (name_video:integer) for labels\n",
        "\n",
        "    iter = 0\n",
        "    #parse files in order to get data as I want\n",
        "    csv_files=os.listdir(path_normal_data)\n",
        "\n",
        "    for i in range(len(csv_files)): \n",
        "        full_filename=csv_files[i]\n",
        "        #print(\"[D] full_filename:\",full_filename)\n",
        "        #bypass files which are not meant to be part of the project\n",
        "        if not (full_filename[:3] == \"out\"):\n",
        "            continue\n",
        "        tmp = []\n",
        "        #print(\"full_filename:\"+full_filename)\n",
        "        with open(path_normal_data+full_filename, \"r\", newline='') as file:\n",
        "            a = file.read().splitlines()\n",
        "            for i in range(len(a)):\n",
        "                tmp.append(a[i].split(\",\"))\n",
        "\n",
        "        #parse values from str to int/float ones\n",
        "        for i in range(len(tmp)):\n",
        "            for k in range(len(tmp[i])):\n",
        "                try: # not very efficient, I know, anyway ... :)\n",
        "                    tmp[i][k] = int(tmp[i][k])\n",
        "                except:\n",
        "                    tmp[i][k] = float(tmp[i][k])\n",
        "\n",
        "        #reshape tmp from 2d to 3d tensor\n",
        "        try:\n",
        "            tmp = np.asarray(tmp)\n",
        "            tmp = np.reshape(tmp,(1,tmp.shape[0],tmp.shape[1]))\n",
        "\n",
        "            if iter == 0:\n",
        "                x_train = tmp #np.asarray(tmp).copy()\n",
        "\n",
        "            else:\n",
        "                x_train = np.append(x_train.copy(),tmp,axis = 0) #np.array([x_train,np.asarray(tmp)]) #[x_train.copy(),np.asarray(tmp).copy()] #\n",
        "        except:\n",
        "            print(\"[E] Bypassing file \"+full_filename+\" due to its length\")\n",
        "            bypassed_videos.append(full_filename)\n",
        "    \n",
        "        iter += 1\n",
        "\n",
        "    x_train = np.asarray(x_train)\n",
        "\n",
        "\n",
        "    #NOTE: first dimension of x_train stands for number of samples in dataset\n",
        "    #      second dim stands for number of features\n",
        "    #      third dim stands for number of packets captured in traffic flow of that sample\n",
        "    print(\"[i] shape initial dataset:\",x_train.shape)\n",
        "\n",
        "    #parse and prepare labels\n",
        "    index_label = 0\n",
        "    for i in range(len(csv_files)):\n",
        "        full_filename=csv_files[i]\n",
        "        #if videos has been bypassed before, bypass also here\n",
        "        if full_filename in bypassed_videos:\n",
        "            continue\n",
        "        #bypass files which are not meant to be part of the project\n",
        "        if not (full_filename[:3] == \"out\"):\n",
        "            continue\n",
        "        real_filename = full_filename.split(\":\")[2][2:-4]\n",
        "        if real_filename in d_labels:\n",
        "            y_train.append(d_labels[real_filename])    \n",
        "        else:\n",
        "            d_labels[real_filename] = index_label #d_labels[real_filename] = int(real_filename[5:7]) #\n",
        "            index_label+=1\n",
        "            y_train.append(d_labels[real_filename])\n",
        "\n",
        "    y_train = np.array(y_train)\n",
        "    num_classes = len(set(y_train))   #n° of different traffic flows (i.e., different videos in dataset)\n",
        "\n",
        "\n",
        "    #shuffle dataset (yes, x_train is whole dataset until now, we make the division\n",
        "    # in validation and test immediately after)\n",
        "    from sklearn.utils import shuffle\n",
        "    x_train, y_train = shuffle(x_train,y_train)\n",
        "\n",
        "    dataset = x_train\n",
        "    label_dataset = y_train\n",
        "\n",
        "\n",
        "    ########## create validation set ##########\n",
        "\n",
        "    #x_valid contains 30% of the elements in x_train\n",
        "    x_valid = x_train[:int(len(x_train)*.3)] \n",
        "    #remove elements in validation set from training one\n",
        "    x_train = x_train[int(len(x_train)*.3):]\n",
        "    #do the same for validation labels\n",
        "    y_valid = y_train[:int(len(y_train)*.3)]\n",
        "    y_train = y_train[int(len(y_train)*.3):]\n",
        "\n",
        "\n",
        "    ########## create test set ##########\n",
        "\n",
        "    #x_test contains 20% of the remaining elements in x_train\n",
        "    x_test = x_train[:int(len(x_train)*.20)] \n",
        "    #remove elements in test set from training one\n",
        "    x_train = x_train[int(len(x_train)*.20):]\n",
        "    #do the same for test labels\n",
        "    y_test = y_train[:int(len(y_train)*.20)]\n",
        "    y_train = y_train[int(len(y_train)*.20):]\n",
        "\n",
        "    #convert val and test labels from list to numpy array\n",
        "    y_valid = np.array(y_valid)\n",
        "    y_test = np.array(y_test)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"[i] labels y_train:\",y_train)\n",
        "    print(\"[i] labels y_valid:\",y_valid)\n",
        "    print(\"[i] labels y_test:\",y_test)\n",
        "    print(\"[i] encoding:\",d_labels)\n",
        "\n",
        "\n",
        "    #set useful values for creating the model\n",
        "    num_samples  =  x_train.shape[0]+x_valid.shape[0]+x_test.shape[0]   #n° of different video samples taken\n",
        "    num_pkt      = len(x_train[0][0])   #n° pkt intercepted in traffic flow (padded to maximum #pkt intercepted among all traffic flows)\n",
        "    num_features = x_train.shape[1]     #n° features (e.g., global_size, pkt_byte, inter-times, ...) \n",
        "\n",
        "    print(\"[D] num_classes:\",num_classes)\n",
        "    print(\"[D] num_samples:\",num_samples)\n",
        "    print(\"[D] num_pkt:\",num_pkt)\n",
        "    print(\"[D] num_features:\",num_features)\n",
        "    print(\"[D] training data:\",x_train.shape[0])\n",
        "\n",
        "\n",
        "    return num_classes, num_samples, num_pkt, num_features, x_train, x_valid, x_test, y_train, y_test, y_valid, d_labels, dataset, label_dataset"
      ],
      "metadata": {
        "id": "3O_Ia3FRUUBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Means"
      ],
      "metadata": {
        "id": "sEtAIVbDMtiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes, num_samples, num_pkt, num_features, x_train, x_valid, x_test, y_train, y_test, y_valid, d_labels, dataset, label_dataset = load_dataset(\"/content/kmeans/\")"
      ],
      "metadata": {
        "id": "4yIv3RMgM2Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reshape dataset from 3D to 2D, standardize features and apply PCA to reduce dimensionality from 20 to 2, in order to be able to visualize data"
      ],
      "metadata": {
        "id": "UryxGRkjRtFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[D] initial dataset.shape:\",dataset.shape)\n",
        "print(\"*\"*80)\n",
        "x_train_2d = None\n",
        "for i in range(len(dataset)):\n",
        "    if i == 0:\n",
        "        x_train_2d = np.array([dataset[i].ravel()])\n",
        "    else:\n",
        "        #convert from 1d to 2d\n",
        "        tmp = np.reshape(dataset[i].ravel(),(1,dataset[i].ravel().shape[0]))\n",
        "        x_train_2d = np.append(x_train_2d,tmp, axis = 0)\n",
        "\n",
        "print(\"[D] final x_train.shape:\",x_train_2d.shape)\n",
        "\n",
        "### scale features ###\n",
        "x_train_2d = StandardScaler().fit_transform(x_train_2d)\n",
        "\n",
        "### apply PCA ###\n",
        "pca_2 = PCA(n_components=2)\n",
        "pca_2_result = pca_2.fit_transform(x_train_2d)\n",
        "print('Explained variation per principal component: {}'.format(pca_2.explained_variance_ratio_))\n",
        "\n",
        "print('Cumulative variance explained by 2 principal components: {:.2%}'.format(np.sum(pca_2.explained_variance_ratio_)))\n",
        "\n",
        "print(\"[D] shape PCA dim2:\",pca_2_result.shape)"
      ],
      "metadata": {
        "id": "e1PTp1g7PGkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"label_dataset:\",label_dataset)\n",
        "print(\"*\"*50)\n",
        "print(\"Encoding:\")\n",
        "for k, v in d_labels.items():\n",
        "    print(\"label\",v,\": \"+k)\n",
        "print(\"*\"*50)\n",
        "\n",
        "#n_clusters = 4\n",
        "\n",
        "\n",
        "for n_clusters in range(2,6):\n",
        "    print(\"+++++++++++ n_clusters:{} +++++++++++\".format(n_clusters))\n",
        "    kmeans = KMeans(n_clusters=n_clusters, n_init=20, algorithm=\"auto\")\n",
        "    # Train K-Means\n",
        "    y_pred_kmeans = kmeans.fit_predict(pca_2_result) #kmeans.fit_predict(x_train_2d)\n",
        "    # Evaluate the K-Means clustering accuracy.\n",
        "\n",
        "    #print(\"y_pred:\",y_pred_kmeans)\n",
        "\n",
        "    '''\n",
        "    ##### plot training set and predicted clusters #####\n",
        "    fig, ax = plt.subplots()\n",
        "    sc = ax.scatter(pca_2_result[:, 0], pca_2_result[:, 1], c=label_dataset, edgecolors='k', cmap=plt.cm.Paired);\n",
        "    ax.legend(*sc.legend_elements(), title='clusters')\n",
        "    plt.xlabel(\"pca 1\")\n",
        "    plt.ylabel(\"pca 2\")\n",
        "    plt.title('Training set')\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    sc = ax.scatter(pca_2_result[:, 0], pca_2_result[:, 1], c=y_pred_kmeans, edgecolors='k');\n",
        "    ax.legend(*sc.legend_elements(), title='clusters')\n",
        "    #get centroids and plot them\n",
        "    centers = np.array(kmeans.cluster_centers_)\n",
        "    plt.scatter(centers[:,0], centers[:,1], marker=\"x\", color='r')\n",
        "    plt.xlabel(\"pca 1\")\n",
        "    plt.ylabel(\"pca 2\")\n",
        "    plt.title('Predicted clusters')\n",
        "    '''\n",
        "    ## print elements in same cluster\n",
        "    for i in range(n_clusters):\n",
        "        print(\"------ Cluster {} ------\".format(i))\n",
        "        #array which contains the indices (of y_train) of the elements which are in cluster {i}\n",
        "        indices_el = [k for k, x in enumerate(y_pred_kmeans) if x == i]\n",
        "        for index in indices_el:\n",
        "            print([i for i in d_labels if d_labels[i] == label_dataset[index]][0][:-2]) \n",
        "        print()\n",
        "\n"
      ],
      "metadata": {
        "id": "gN_Q1ygWRsqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Attacker's DL Model**\n",
        "\n",
        "TODO: brief description what the model is used for and how it works"
      ],
      "metadata": {
        "id": "TDJ5xM0GH7nc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload - Parse and Load dataset with traffic without defence\n",
        "Following cell is needed in order to upload on colab the files which contain the values of the features, to parse the data in the format we want and to finally split the dataset into train, validation and test sets."
      ],
      "metadata": {
        "id": "ZgN7AgVTbGI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes, num_samples, num_pkt, num_features, x_train, x_valid, x_test, y_train, y_test, y_valid, d_labels, dataset, label_dataset = load_dataset(\"/content/out/\")"
      ],
      "metadata": {
        "id": "yl1cjw3RmCBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Model\n",
        "TODO: describe how the model is composed + activation functions + loss"
      ],
      "metadata": {
        "id": "3iCQef110Kk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Best model found\n",
        "def build_model_2d(num_classes, rows, cols, nb_filters=64, pool_size=[2,2], kernel_size=[1,3]):\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.Conv2D(nb_filters, kernel_size=kernel_size, input_shape = (rows, cols,1)))\n",
        "    model.add(keras.layers.Activation('tanh'))\n",
        "    model.add(keras.layers.Conv2D(nb_filters, kernel_size=kernel_size))\n",
        "    model.add(keras.layers.Activation('relu'))\n",
        "    model.add(keras.layers.Conv2D(nb_filters, kernel_size=kernel_size))\n",
        "    model.add(keras.layers.Dropout(rate=0.4))\n",
        "    model.add(keras.layers.Activation('relu'))\n",
        "    model.add(keras.layers.Conv2D(nb_filters, kernel_size=kernel_size))\n",
        "    model.add(keras.layers.Dropout(rate=0.7))\n",
        "    model.add(keras.layers.Activation('relu'))\n",
        "    model.add(keras.layers.MaxPooling2D(pool_size=pool_size))\n",
        "    model.add(keras.layers.Conv2D(nb_filters, kernel_size=kernel_size))\n",
        "    model.add(keras.layers.Dropout(rate=0.2))\n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(64))\n",
        "    model.add(keras.layers.Activation('relu'))\n",
        "    model.add(keras.layers.Dropout(rate=0.4))\n",
        "    model.add(keras.layers.Dense(num_classes))\n",
        "    model.add(keras.layers.Activation('softmax'))\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "        optimizer='adam', metrics=['accuracy']) #optimizer='sgd'\n",
        "    return model\n",
        "\n",
        "\n",
        "model = build_model_2d(num_classes=num_classes,rows=num_features,cols=num_pkt)\n",
        "\n",
        "#print(get_model_summary(model))\n",
        "\n",
        "#from keras.utils.vis_utils import plot_model\n",
        "#plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True, rankdir=\"LR\")"
      ],
      "metadata": {
        "id": "nyKCTMzx0fwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model\n",
        "TODO: describe main characteristics of the training session (e.g., #epochs, hyperparamters, ...)"
      ],
      "metadata": {
        "id": "r3612AwkDYZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss(history):\n",
        "  plt.figure(figsize=(10,6)) \n",
        "  plt.plot(history.epoch, history.history['loss'], label='loss')\n",
        "  plt.plot(history.epoch, history.history['val_loss'], label='val_loss')\n",
        "  plt.legend()\n",
        "  plt.title('loss')\n",
        "\n",
        "def plot_accuracy(history):\n",
        "  plt.figure(figsize=(10,6))\n",
        "  plt.plot(history.epoch,history.history['accuracy'],label='accuracy')\n",
        "  plt.plot(history.epoch,history.history['val_accuracy'],label='val_accuracy')\n",
        "  plt.legend()\n",
        "  plt.title('accuracy')\n",
        "\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=20, batch_size=10\n",
        "                    ,validation_data=(x_valid, y_valid))\n",
        "plot_loss(history)\n",
        "plot_accuracy(history)"
      ],
      "metadata": {
        "id": "TTnEZnhmDlMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test model"
      ],
      "metadata": {
        "id": "DcxqbEa7eqQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(\"*\"*80)\n",
        "print(\" %s test_set: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "metadata": {
        "id": "U_TZ3SE5esNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot confusion matrix without defences"
      ],
      "metadata": {
        "id": "LP8KMQPuPbCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = 0\n",
        "w = 0\n",
        "y_pred = []\n",
        "\n",
        "for i in range(1,len(x_test)+1):\n",
        "    prediction = model.predict(x_test[i-1:i]) #prediction contains probability that the sample is assigned to each label by the model\n",
        "    pred_label = np.where(prediction[0] == max(prediction[0]))[0][0]\n",
        "    true_label = y_test[i-1]\n",
        "    y_pred.append(pred_label)\n",
        "    if int(true_label) == int(pred_label):\n",
        "        t += 1\n",
        "        true_video = list(d_labels.keys())[list(d_labels.values()).index(int(true_label))]\n",
        "        #print(\"OK \"+true_video)\n",
        "        #print(\"label:\"+str(true_label))\n",
        "    else:\n",
        "        w += 1\n",
        "        print(\"[-] NOT OK\")\n",
        "        print(\"prediction:\",np.round(prediction,decimals=3))\n",
        "        print(\"true label:\"+str(true_label))\n",
        "        print(\"pred label:\"+str(pred_label))\n",
        "        true_video = list(d_labels.keys())[list(d_labels.values()).index(int(true_label))]\n",
        "        pred_video = list(d_labels.keys())[list(d_labels.values()).index(int(pred_label))]\n",
        "        print(\"\\'\"+true_video + \"\\'\" + \" predicted as \\'\" + pred_video + \"\\'\")\n",
        "        print(\"-\"*80)\n",
        "    \n",
        "\n",
        "#print(\"acc:\"+str(float(t/len(x_test))))\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "df_cm = pd.DataFrame(cm, index=[i for i in range(num_classes)],\n",
        "                     columns = [i for i in range(num_classes)])\n",
        "plt.figure(figsize = (10,7))\n",
        "seaborn.heatmap(df_cm, annot=True, cmap=\"rainbow\", linewidths=.5).set(xlabel='Predicted label', ylabel='True label', title=\"Confusion matrix w/out defence\")\n",
        "\n",
        "for k, v in d_labels.items():\n",
        "    print(\"label\",v,\": \"+k)\n",
        "print()"
      ],
      "metadata": {
        "id": "ekYYF38QMyiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MhNFxI6R7vUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Divergent**"
      ],
      "metadata": {
        "id": "FZCJaX71BSXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload - Parse and Load dataset with Divergent applied"
      ],
      "metadata": {
        "id": "oZSDlU9FRUta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rd out_div/\n",
        "!mkdir out_div/"
      ],
      "metadata": {
        "id": "8D_hIhV1uSbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes, num_samples, num_pkt, num_features, x_train, x_valid, x_test, y_train, y_test, y_valid, d_labels, dataset, label_dataset = load_dataset(\"/content/out_div/\")"
      ],
      "metadata": {
        "id": "G92d40bRReEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model on traffic with Divergent"
      ],
      "metadata": {
        "id": "YppH7fkHgYW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model_2d(num_classes, rows, cols, nb_filters=64, pool_size=[2,2], kernel_size=[1,3]):\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.Conv2D(nb_filters, kernel_size=kernel_size, input_shape = (rows, cols,1)))\n",
        "    model.add(keras.layers.Activation('tanh'))\n",
        "    model.add(keras.layers.Conv2D(nb_filters, kernel_size=kernel_size))\n",
        "    model.add(keras.layers.Activation('relu'))\n",
        "    model.add(keras.layers.Conv2D(nb_filters, kernel_size=kernel_size))\n",
        "    model.add(keras.layers.Dropout(rate=0.4))\n",
        "    model.add(keras.layers.Activation('relu'))\n",
        "    model.add(keras.layers.Conv2D(nb_filters, kernel_size=kernel_size))\n",
        "    model.add(keras.layers.Dropout(rate=0.7))\n",
        "    model.add(keras.layers.Activation('relu'))\n",
        "    model.add(keras.layers.MaxPooling2D(pool_size=pool_size))\n",
        "    model.add(keras.layers.Conv2D(nb_filters, kernel_size=kernel_size))\n",
        "    model.add(keras.layers.Dropout(rate=0.2))\n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(64))\n",
        "    model.add(keras.layers.Activation('relu'))\n",
        "    model.add(keras.layers.Dropout(rate=0.7))\n",
        "    model.add(keras.layers.Dense(num_classes))\n",
        "    model.add(keras.layers.Activation('softmax'))\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "        optimizer='adam', metrics=['accuracy']) #optimizer='sgd'\n",
        "    return model\n",
        "\n",
        "\n",
        "model = build_model_2d(num_classes=num_classes,rows=num_features,cols=num_pkt)\n",
        "\n",
        "\n",
        "\n",
        "######################################################################################\n",
        "\n",
        "def plot_loss(history):\n",
        "  plt.figure(figsize=(10,6)) \n",
        "  plt.plot(history.epoch, history.history['loss'], label='loss')\n",
        "  plt.plot(history.epoch, history.history['val_loss'], label='val_loss')\n",
        "  plt.legend()\n",
        "  plt.title('loss')\n",
        "\n",
        "def plot_accuracy(history):\n",
        "  plt.figure(figsize=(10,6))\n",
        "  plt.plot(history.epoch,history.history['accuracy'],label='accuracy')\n",
        "  plt.plot(history.epoch,history.history['val_accuracy'],label='val_accuracy')\n",
        "  plt.legend()\n",
        "  plt.title('accuracy')\n",
        "\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=30, batch_size=10\n",
        "                    ,validation_data=(x_valid, y_valid))\n",
        "plot_loss(history)\n",
        "plot_accuracy(history)\n",
        "\n",
        "\n",
        "######################################################################################\n",
        "\n",
        "\n",
        "scores = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(\"\\n\"+\"-\"*90+\"\\n\")\n",
        "print(\"[+] %s test_set: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "metadata": {
        "id": "2aYuuC4qgbqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot confusion matrix with Divergent applied"
      ],
      "metadata": {
        "id": "EkzhpQwLDE9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = 0\n",
        "w = 0\n",
        "y_pred = []\n",
        "\n",
        "for i in range(1,len(x_test)+1):\n",
        "    prediction = model.predict(x_test[i-1:i]) #prediction contains probability that the sample is assigned to each label by the model\n",
        "    \n",
        "    pred_label = np.where(prediction[0] == max(prediction[0]))[0][0]\n",
        "    true_label = y_test[i-1]\n",
        "    y_pred.append(pred_label)\n",
        "    if int(true_label) == int(pred_label):\n",
        "        t += 1\n",
        "        true_video = list(d_labels.keys())[list(d_labels.values()).index(int(true_label))]\n",
        "        print(\"[+] OK \"+true_video)\n",
        "        print(\"prediction:\",np.round(prediction,decimals=3))\n",
        "        print(\"label:\"+str(true_label))\n",
        "        print(\"-\"*80)\n",
        "    else:\n",
        "        w += 1\n",
        "        true_video = list(d_labels.keys())[list(d_labels.values()).index(int(true_label))]\n",
        "        pred_video = list(d_labels.keys())[list(d_labels.values()).index(int(pred_label))]\n",
        "        print(\"[-] NOT OK\")\n",
        "        print(\"prediction:\",np.round(prediction,decimals=3))\n",
        "        print(\"true label:\"+str(true_label))\n",
        "        print(\"pred label:\"+str(pred_label))\n",
        "        print(\"\\'\"+true_video + \"\\'\" + \" predicted as \\'\" + pred_video + \"\\'\")\n",
        "        print(\"-\"*80)\n",
        "    \n",
        "\n",
        "#print(\"acc:\"+str(float(t/len(x_test))))\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "df_cm = pd.DataFrame(cm, index=[i for i in range(num_classes)],\n",
        "                     columns = [i for i in range(num_classes)])\n",
        "plt.figure(figsize = (10,7))\n",
        "seaborn.heatmap(df_cm, annot=True, cmap=\"rainbow\", linewidths=.5).set(xlabel='Predicted label', ylabel='True label',title=\"Confusion matrix w/ Divergent\")\n",
        "\n",
        "for k, v in d_labels.items():\n",
        "    print(\"label\",v,\": \"+k)\n",
        "print()"
      ],
      "metadata": {
        "id": "fk4x-6A9S4X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment with **TOR** traffic\n"
      ],
      "metadata": {
        "id": "a8JHRbRwAURN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create directory to put dataset\n",
        "!mkdir tor_traffic"
      ],
      "metadata": {
        "id": "-FdS2RmYAeMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### \n",
        "# upload files directly from tab on \n",
        "#   the left\n",
        "####\n",
        "def load_TOR_dataset(path):\n",
        "    '''\n",
        "    @arg: path contains the path in which there are the \n",
        "        TOR files to be parsed, loaded and split\n",
        "\n",
        "    @return: num_classes\n",
        "             num_samples\n",
        "             num_pkt\n",
        "             num_features\n",
        "             x_train\n",
        "             x_valid\n",
        "             x_test\n",
        "             y_train\n",
        "             y_test\n",
        "             y_valid\n",
        "             dataset\n",
        "             label_dataset\n",
        "    '''\n",
        "    x_train = None\n",
        "    x_valid = None\n",
        "    x_test  = None\n",
        "    y_train = []\n",
        "    y_test  = []\n",
        "    y_valid = []\n",
        "\n",
        "    MAX_SIZE_CELL = 25000 #28000\n",
        "\n",
        "    cell_files=os.listdir(path)\n",
        "    #print(cell_files)\n",
        "    iter = 0\n",
        "\n",
        "    for cell_name in cell_files:\n",
        "        ## add values to x_train\n",
        "        tmp = [[],[]]\n",
        "        #print(\"cell_name:\",cell_name)\n",
        "        with open(path+cell_name, \"r\", newline='') as file:\n",
        "            cell_values = file.read().splitlines()#.split(\"\\t\")\n",
        "            #print(\"cell_values:\",cell_values)\n",
        "            for value in cell_values:\n",
        "                value = value.split(\"\\t\")\n",
        "                #print(\"value:\",value)\n",
        "                tmp[0].append(float(value[0])) #appending timestamps\n",
        "                tmp[1].append(int(value[1])) #appending direction packets\n",
        "            \n",
        "            #append dummy values so that all cells have same dimensions\n",
        "            for i in range(MAX_SIZE_CELL - len(tmp[0])):\n",
        "                tmp[0].append(-20)\n",
        "                tmp[1].append(-20)\n",
        "\n",
        "            tmp = np.asarray(tmp)\n",
        "            tmp[0] = np.asarray(tmp[0])\n",
        "            tmp[1] = np.asarray(tmp[1])\n",
        "            #reshape tmp from 2D to 3D\n",
        "            tmp = np.reshape(tmp,(1,tmp.shape[0],tmp.shape[1]))\n",
        "\n",
        "            if iter == 0:\n",
        "                x_train = tmp\n",
        "            else:\n",
        "                x_train = np.append(x_train.copy(),tmp,axis = 0)#x_train.append(tmp)\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        ## add labels to y_train\n",
        "        y_train.append(int(cell_name[0]))\n",
        "\n",
        "        #if iter == 15:\n",
        "        #    break   \n",
        "    \n",
        "    y_train = np.array(y_train)\n",
        "    \n",
        "    \n",
        "    x_train = np.asarray(x_train)\n",
        "    #print(\"x_train:\",x_train)\n",
        "\n",
        "    num_classes = len(set(y_train))\n",
        "\n",
        "    #shuffle dataset (yes, x_train is whole dataset until now, we make the division\n",
        "    # in validation and test immediately after)\n",
        "    from sklearn.utils import shuffle\n",
        "    x_train, y_train = shuffle(x_train,y_train)\n",
        "\n",
        "    dataset = x_train\n",
        "    label_dataset = y_train\n",
        "\n",
        "\n",
        "    ########## create validation set ##########\n",
        "\n",
        "    #x_valid contains 30% of the elements in x_train\n",
        "    x_valid = x_train[:int(len(x_train)*.3)]\n",
        "    #remove elements in validation set from training one\n",
        "    x_train = x_train[int(len(x_train)*.3):]\n",
        "    #do the same for validation labels\n",
        "    y_valid = y_train[:int(len(y_train)*.3)]\n",
        "    y_train = y_train[int(len(y_train)*.3):]\n",
        "\n",
        "\n",
        "    ########## create test set ##########\n",
        "\n",
        "    #x_test contains 20% of the remaining elements in x_train\n",
        "    x_test = x_train[:int(len(x_train)*.20)]\n",
        "    #remove elements in test set from training one\n",
        "    x_train = x_train[int(len(x_train)*.20):]\n",
        "    #do the same for test labels\n",
        "    y_test = y_train[:int(len(y_train)*.20)]\n",
        "    y_train = y_train[int(len(y_train)*.20):]\n",
        "\n",
        "    #convert val and test labels from list to numpy array\n",
        "    y_valid = np.array(y_valid)\n",
        "    y_test = np.array(y_test)\n",
        "\n",
        "\n",
        "    print(\"[i] labels y_train:\",y_train)\n",
        "    print(\"[i] labels y_valid:\",y_valid)\n",
        "    print(\"[i] labels y_test:\",y_test)\n",
        "\n",
        "    #set useful values for creating the model\n",
        "    num_samples  =  x_train.shape[0]+x_valid.shape[0]+x_test.shape[0]   #n° of different video samples taken\n",
        "    num_pkt      = len(x_train[0][0])   #n° pkt intercepted in traffic flow (padded to maximum #pkt intercepted among all traffic flows)\n",
        "    num_features = x_train.shape[1]     #n° features (e.g., global_size, pkt_byte, inter-times, ...)\n",
        "\n",
        "    print(\"[D] num_classes:\",num_classes)\n",
        "    print(\"[D] num_samples:\",num_samples)\n",
        "    print(\"[D] max num_pkt:\",num_pkt)\n",
        "    print(\"[D] num_features:\",num_features)\n",
        "    print(\"[D] training data:\",x_train.shape[0])\n",
        "\n",
        "    \n",
        "    return num_classes, num_samples, num_pkt, num_features, x_train, x_valid, x_test, y_train, y_test, y_valid, dataset, label_dataset"
      ],
      "metadata": {
        "id": "xIFUpLdUAvAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing on cells without defence"
      ],
      "metadata": {
        "id": "O_ioMnNgM73n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes, num_samples, num_pkt, num_features, x_train, x_valid, x_test, y_train, y_test, y_valid, dataset, label_dataset = load_TOR_dataset(\"tor_traffic/\")"
      ],
      "metadata": {
        "id": "gRtN32YsEqo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Best model found for TOR traffic, with 1111 samples in dataset, 10 classes and 623 sampled in training set\n",
        "def build_model_2d(num_classes, rows, cols, nb_filters=32, pool_size=[2,2], kernel_size=[1,3]):\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.Conv2D(nb_filters, kernel_size=kernel_size, input_shape = (rows, cols,1)))\n",
        "    model.add(keras.layers.Activation('relu'))\n",
        "    #model.add(keras.layers.Conv2D(nb_filters, kernel_size=kernel_size))\n",
        "    #model.add(keras.layers.Activation('tanh'))\n",
        "    model.add(keras.layers.Conv2D(nb_filters, kernel_size=kernel_size))\n",
        "    model.add(keras.layers.Dropout(rate=0.4))\n",
        "    model.add(keras.layers.Activation('tanh'))\n",
        "    model.add(keras.layers.Conv2D(nb_filters, kernel_size=kernel_size))\n",
        "    model.add(keras.layers.Dropout(rate=0.7))\n",
        "    model.add(keras.layers.Activation('relu'))\n",
        "    model.add(keras.layers.AveragePooling2D(pool_size=pool_size))#model.add(keras.layers.MaxPooling2D(pool_size=pool_size))\n",
        "    model.add(keras.layers.Conv2D(nb_filters, kernel_size=kernel_size))\n",
        "    model.add(keras.layers.Dropout(rate=0.2))\n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(128))\n",
        "    model.add(keras.layers.Activation('relu'))\n",
        "    model.add(keras.layers.Dense(64))\n",
        "    model.add(keras.layers.Activation('relu'))\n",
        "    model.add(keras.layers.Dense(32))\n",
        "    model.add(keras.layers.Activation('relu'))\n",
        "    model.add(keras.layers.Dropout(rate=0.4))\n",
        "    model.add(keras.layers.Dense(num_classes))\n",
        "    model.add(keras.layers.Activation('softmax'))\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "        optimizer='adam', metrics=['accuracy']) #optimizer='sgd'\n",
        "    return model\n",
        "\n",
        "\n",
        "model = build_model_2d(num_classes=num_classes,rows=num_features,cols=num_pkt)\n",
        "\n",
        "#print(get_model_summary(model))\n",
        "\n",
        "from keras.utils.vis_utils import plot_model\n",
        "#plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True, rankdir=\"LR\")\n",
        "\n",
        "\n",
        "\n",
        "def plot_loss(history):\n",
        "  plt.figure(figsize=(10,6)) \n",
        "  plt.plot(history.epoch, history.history['loss'], label='loss')\n",
        "  plt.plot(history.epoch, history.history['val_loss'], label='val_loss')\n",
        "  plt.legend()\n",
        "  plt.title('loss')\n",
        "\n",
        "def plot_accuracy(history):\n",
        "  plt.figure(figsize=(10,6))\n",
        "  plt.plot(history.epoch,history.history['accuracy'],label='accuracy')\n",
        "  plt.plot(history.epoch,history.history['val_accuracy'],label='val_accuracy')\n",
        "  plt.legend()\n",
        "  plt.title('accuracy')\n",
        "\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=45, batch_size=25\n",
        "                    ,validation_data=(x_valid, y_valid))\n",
        "plot_loss(history)\n",
        "plot_accuracy(history)\n",
        "\n",
        "\n",
        "\n",
        "scores = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(\"*\"*80)\n",
        "print(\" %s test_set: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "metadata": {
        "id": "Bp1HktdjF5iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing on cells with Divergent"
      ],
      "metadata": {
        "id": "ip9pK2tYLPTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes, num_samples, num_pkt, num_features, x_train, x_valid, x_test, y_train, y_test, y_valid, dataset, label_dataset = load_TOR_dataset(\"tor_traffic/\")"
      ],
      "metadata": {
        "id": "k82AWBL-nLsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model_2d(num_classes, rows, cols, nb_filters=32, pool_size=[2,2], kernel_size=[1,3]):\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.Conv2D(nb_filters, kernel_size=kernel_size, input_shape = (rows, cols,1)))\n",
        "    model.add(keras.layers.Activation('relu'))\n",
        "    model.add(keras.layers.Conv2D(nb_filters, kernel_size=kernel_size))\n",
        "    model.add(keras.layers.Dropout(rate=0.4))\n",
        "    model.add(keras.layers.Activation('tanh'))\n",
        "    model.add(keras.layers.Conv2D(nb_filters, kernel_size=kernel_size))\n",
        "    model.add(keras.layers.Dropout(rate=0.7))\n",
        "    model.add(keras.layers.Activation('relu'))\n",
        "    model.add(keras.layers.AveragePooling2D(pool_size=pool_size))#model.add(keras.layers.MaxPooling2D(pool_size=pool_size))\n",
        "    model.add(keras.layers.Conv2D(nb_filters, kernel_size=kernel_size))\n",
        "    model.add(keras.layers.Dropout(rate=0.2))\n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(128))\n",
        "    model.add(keras.layers.Activation('relu'))\n",
        "    model.add(keras.layers.Dense(64))\n",
        "    model.add(keras.layers.Activation('relu'))\n",
        "    model.add(keras.layers.Dense(32))\n",
        "    model.add(keras.layers.Activation('relu'))\n",
        "    model.add(keras.layers.Dropout(rate=0.4))\n",
        "    model.add(keras.layers.Dense(num_classes))\n",
        "    model.add(keras.layers.Activation('softmax'))\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "        optimizer='adam', metrics=['accuracy']) #optimizer='sgd'\n",
        "    return model\n",
        "\n",
        "\n",
        "model = build_model_2d(num_classes=num_classes,rows=num_features,cols=num_pkt)\n",
        "\n",
        "\n",
        "def plot_loss(history):\n",
        "  plt.figure(figsize=(10,6)) \n",
        "  plt.plot(history.epoch, history.history['loss'], label='loss')\n",
        "  plt.plot(history.epoch, history.history['val_loss'], label='val_loss')\n",
        "  plt.legend()\n",
        "  plt.title('loss')\n",
        "\n",
        "def plot_accuracy(history):\n",
        "  plt.figure(figsize=(10,6))\n",
        "  plt.plot(history.epoch,history.history['accuracy'],label='accuracy')\n",
        "  plt.plot(history.epoch,history.history['val_accuracy'],label='val_accuracy')\n",
        "  plt.legend()\n",
        "  plt.title('accuracy')\n",
        "\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=35, batch_size=25\n",
        "                    ,validation_data=(x_valid, y_valid))\n",
        "plot_loss(history)\n",
        "plot_accuracy(history)\n",
        "\n",
        "\n",
        "\n",
        "scores = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(\"*\"*80)\n",
        "print(\" %s test_set: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "metadata": {
        "id": "DgwroILxl84z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = 0\n",
        "w = 0\n",
        "y_pred = []\n",
        "\n",
        "for i in range(1,len(x_test)+1):\n",
        "    prediction = model.predict(x_test[i-1:i]) #prediction contains probability that the sample is assigned to each label by the model\n",
        "    #print(\"prediction:\",np.round(prediction,decimals=3))\n",
        "    pred_label = np.where(prediction[0] == max(prediction[0]))[0][0]\n",
        "    true_label = y_test[i-1]\n",
        "    y_pred.append(pred_label)\n",
        "    if int(true_label) == int(pred_label):\n",
        "        t += 1\n",
        "        #print(\"OK \"+true_video)\n",
        "        #print(\"label:\"+str(true_label))\n",
        "    else:\n",
        "        w += 1\n",
        "        #print(\"[-] NOT OK\")\n",
        "        #print(\"true label:\"+str(true_label))\n",
        "        #print(\"pred label:\"+str(pred_label))\n",
        "        #print(\"-\"*80)\n",
        "    \n",
        "\n",
        "#print(\"acc:\"+str(float(t/len(x_test))))\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "df_cm = pd.DataFrame(cm, index=[i for i in range(num_classes)],\n",
        "                     columns = [i for i in range(num_classes)])\n",
        "plt.figure(figsize = (10,7))\n",
        "seaborn.heatmap(df_cm, annot=True, cmap=\"rainbow\", linewidths=.5).set(xlabel='Predicted label', ylabel='True label',title=\"Confusion matrix w/ Divergent in Tor traffic\")\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "id": "EDlmc4N6pnOy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}